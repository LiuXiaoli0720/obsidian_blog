  
**文献阅读：**  
- QC-A2S: ACHIEVE LATENCY-EFFICIENT TEMPORA-CODING SPIKINGLLMS VIADISCRETIZATION-AWARECON VERSION  
- ONE-TIMESTEP IS ENOUGH: ACHIEVING HIGH PERFORMANCE ANN-TO-SNN CONVERSION VIA SCALE-AND-FIRE NEURONS  
- TTFSFormer: A TTFS-based Lossless Conversion of Spiking Transformer  
- FAS: Fast ANN-SNN Conversion for Spiking Large Language Models  
- Energy-Efficient and Dequantization-Free Q-LLMs: A Spiking Neural Network Approach to Salient Value Mitigation  
- SpikingMamba: Towards Energy-Efficient Large Language Models via Knowledge Distillation from Mamba  
- LLaMA: Open and Efficient Foundation Language Models  
- LLaMA 2: Open Foundation and Fine-Tuned Chat Models  
- LLaMA 3: TheLlama3HerdofModels  
  
